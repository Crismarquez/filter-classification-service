{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086f0993-f945-4486-bd78-94ac02963ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0af079-87e5-4df9-8ab8-48de78487242",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0398b748-11a7-40e1-b3e3-45690efa3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "ENV_VARIABLES = {\n",
    "    **dotenv_values(str(BASE_DIR / \".env\")),  # load environment variables from .env file\n",
    "    #**os.environ,  # override loaded values with environment variables\n",
    "}\n",
    "\n",
    "GERENCIAL_REPORT_NAME = \"gerencial\"\n",
    "DETAIL_REPORT_NAME = \"detallado\"\n",
    "\n",
    "gpt4_vision_key = ENV_VARIABLES[\"AZURE_OPENAI_KEY\"]\n",
    "gpt4_vision_endpoint = f\"https://{ENV_VARIABLES['AZURE_OPENAI_SERVICE']}.openai.azure.com\"\n",
    "gpt4_vision_version = \"2024-04-01-preview\"\n",
    "gpt4_vision_name = ENV_VARIABLES['AZURE_OPENAI_MODEL_GPT4O']\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.2,\n",
    "    openai_api_key=gpt4_vision_key,\n",
    "    api_version=gpt4_vision_version,\n",
    "    azure_endpoint=gpt4_vision_endpoint,\n",
    "    # openai_api_version = openai.api_version,\n",
    "    azure_deployment=gpt4_vision_name,\n",
    "    max_tokens=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336fbd3-2aee-486f-b350-c22a0ee046d2",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865eb6b4-0d59-427f-af5b-8ca5414e2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image_path):\n",
    "    # Abre la imagen\n",
    "    with Image.open(image_path) as image:\n",
    "        # Convierte la imagen a un objeto BytesIO\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        # Codifica la imagen a base64\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    return img_str\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        \n",
    "def generate_sha256_hash(input_string):\n",
    "    # Convertir el string a bytes\n",
    "    byte_string = input_string.encode('utf-8')\n",
    "    \n",
    "    # Crear un objeto de hash SHA-256\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    \n",
    "    # Actualizar el objeto de hash con los datos en bytes\n",
    "    sha256_hash.update(byte_string)\n",
    "    \n",
    "    # Obtener el hash final en formato hexadecimal\n",
    "    hex_digest = sha256_hash.hexdigest()\n",
    "    \n",
    "    return hex_digest\n",
    "\n",
    "def sort_key(path):\n",
    "    # Extraer la parte numérica del nombre del archivo después del guion bajo\n",
    "    name = path.stem  # Obtiene el nombre del archivo sin la extensión\n",
    "    parts = name.split(\"_\")\n",
    "    return int(parts[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1f76c-0f16-4c73-adc3-8c2b534da927",
   "metadata": {},
   "source": [
    "## prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4592f-ac82-498f-bdea-00769c385575",
   "metadata": {},
   "source": [
    "### for report type: gerencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6c4cc42c-a677-43cb-9f70-ddbaa0ad842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_system_prompt = \"\"\"\n",
    "You are a vision model designed to operate in an autonomous store. Your goal is to analyze the images captured in the store and list information about customer actions. Your specific tasks include:\n",
    "\n",
    "Identify how many possible take actions exist in the image sequence.\n",
    "write very consice conclusion where indicate posible actions and products related\n",
    "\n",
    "IMPORTANT: ## Focus on Hand Movements:\n",
    "Concentrate on the movement of the customer's hands.\n",
    "Identify the objects being held in the moving hand.\n",
    "\n",
    "format:\n",
    "1. product taked, it seem a ..\n",
    "2. product taked, it seem a ..\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are a vision model designed to operate in an autonomous store. Your goal is to analyze the images captured in the store and list information about customer actions. Your specific tasks include:\n",
    "\n",
    "## Action Identification:\n",
    "1. Detect if a customer is taking a product from a shelf.\n",
    "2. Detect if a customer is returning a product to a shelf.\n",
    "\n",
    "## Action Listing:\n",
    "1. List all detected actions of taking or returning products.\n",
    "2. Specify which product is being taken or returned.\n",
    "\n",
    "## Additional Guidelines:\n",
    " - Provide high accuracy in identifying customer actions.\n",
    " - You must include all action detected.\n",
    " - Ensure clear differentiation between the actions of taking and returning products.\n",
    " - Generate real-time reports for each detected action, including a summary of the product involved.\n",
    " - Maintain a log of the time and location within the store where the actions were detected.\n",
    "\"\"\"\n",
    "\n",
    "final_description_system_prompt = \"\"\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a511d637-3a31-45f9-a373-b07e9b23f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(BaseModel):\n",
    "    \"\"\"Information about an action identified.\"\"\"\n",
    "\n",
    "    action: str = Field(default=None, description=\"Action detected taking/returning\")\n",
    "    product: str = Field(default=None, description=\"product detected in the action. products allowed: [red-beer, yellow-beer, gray-beer, chips]\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about actions.\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    actions: List[Action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37d793-47d4-4113-975d-50ee1456f0de",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c9c9f90a-e5e1-4910-9cc9-c03ae07f528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load sources\n",
    "store_session = \"store0011235537879556\"\n",
    "images_dir = DATA_DIR / store_session / \"imagenes_camaras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ac7d1795-fbcf-4d7e-a29f-c19a589471fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_range = [(20, 60), (61, 100)] #(61, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "f15eb938-2a23-405a-a557-24b92042f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_structured =  llm.with_structured_output(schema=Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "42769597-a1e5-405c-a172-851579b6bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for camera 0\n",
      "Analysis 1. Product taken, it seems to be a can of soda.\n",
      "2. Product taken, it seems to be another can of soda.\n",
      "Predicting for camera 1\n",
      "Analysis 1. Product taken, it seems a small yellow and red item (possibly a snack).\n",
      "2. Product taken, it seems a black and red can (possibly a beverage).\n",
      "3. Product taken, it seems a red and white can (possibly a beverage).\n",
      "Predicting for camera 2\n",
      "Analysis 1. Product taken, it seems a red can.\n",
      "2. Product taken, it seems a red can.\n",
      "Predicting for camera 0\n",
      "Analysis 1. Product taken, it seems to be a can of \"Speed\" energy drink.\n",
      "2. Product taken, it seems to be a bag of chips.\n",
      "Predicting for camera 1\n",
      "Analysis 1. Product taken, it seems a yellow snack pack.\n",
      "2. Product taken, it seems a green can (Speed energy drink).\n",
      "3. Product taken, it seems a blue and white snack pack.\n",
      "Predicting for camera 2\n",
      "Analysis 1. Product taken, it seems to be a yellow can.\n",
      "2. Product taken, it seems to be a blue and white bag.\n"
     ]
    }
   ],
   "source": [
    "predictions_by_cameras = {\n",
    "    0: [],\n",
    "    1: [],\n",
    "    2: []\n",
    "}\n",
    "for range_ob in filter_range:\n",
    "    images_camera = {\n",
    "    0: [],\n",
    "    1: [],\n",
    "    2: []\n",
    "}\n",
    "\n",
    "    # get images by camera\n",
    "    for image_dir in images_dir.iterdir():\n",
    "        name = image_dir.stem\n",
    "        camera = int(name.split(\"_\")[0])\n",
    "        n_frame = int(name.split(\"_\")[1])\n",
    "        if n_frame > range_ob[0] and n_frame < range_ob[1]:\n",
    "            images_camera[camera].append(image_dir)\n",
    "    for i in range(3):\n",
    "        images_camera[i] = sorted(images_camera[i], key=sort_key)\n",
    "\n",
    "    # predict by camera\n",
    "    for camera in images_camera.keys():\n",
    "        print(f\"Predicting for camera {camera}\")\n",
    "        # select sample - skip 2\n",
    "        sample = [path for i, path in enumerate(images_camera[camera]) if i % 2 == 0]\n",
    "\n",
    "        base64_images = []\n",
    "        for image_file_dir in sample:\n",
    "            base64_images.append(encode_image(image_file_dir))\n",
    "        \n",
    "        imgs_content = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_bs64}\"}} for image_bs64 in base64_images]\n",
    "\n",
    "        # generate first escene description\n",
    "        first_description = llm.invoke(\n",
    "                    [\n",
    "                        HumanMessage(\n",
    "                        content=[\n",
    "                            {\"type\": \"text\", \"text\": reflection_system_prompt}] + imgs_content\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "        \n",
    "        print(f\"Analysis {first_description.content}\")\n",
    "        # generate structured output\n",
    "        prev_analysis = [{\"type\": \"text\", \"text\": f\"## scene analysis:\\n {first_description.content}\\n Start to generate the structured output\"}]\n",
    "        structured_output = llm_structured.invoke(\n",
    "                    [\n",
    "                        HumanMessage(\n",
    "                        content=[\n",
    "                            {\"type\": \"text\", \"text\": sys_prompt}] + imgs_content + prev_analysis\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        actions_detected = structured_output.actions\n",
    "        predictions_by_cameras[camera].extend(actions_detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e401a5ee-cc6b-43d1-8818-ad77db96531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result = {}\n",
    "\n",
    "for i in range(3):\n",
    "    fmodel_predictions = predictions_by_cameras[i]\n",
    "    clean_pred = []\n",
    "    for item in fmodel_predictions:\n",
    "        clean_pred.append(\n",
    "            {\n",
    "                \"action\": item.action,\n",
    "                \"product\": item.product\n",
    "            }\n",
    "        )\n",
    "    prediction_result[i] = clean_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d9f1fd16-31d5-4409-95a9-f4c6ac6d5a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'action': 'taking', 'product': 'gray-beer'},\n",
       "  {'action': 'taking', 'product': 'gray-beer'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'chips'}],\n",
       " 1: [{'action': 'taking', 'product': 'chips'},\n",
       "  {'action': 'taking', 'product': 'red-beer'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'chips'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'gray-beer'}],\n",
       " 2: [{'action': 'taking', 'product': 'red-beer'},\n",
       "  {'action': 'taking', 'product': 'red-beer'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'chips'}]}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "8b8d8c3c-0fb2-46de-a1e4-da93c33efc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "with open(DATA_DIR / 'prediction_result.json', 'w') as file:\n",
    "    json.dump(prediction_result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c9f84712-6d27-4167-bdf3-41b9d7b1deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(DATA_DIR / 'prediction_result.json', 'r') as file:\n",
    "    prediction_result = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3d8fd875-d996-4567-8abc-2f4661cb491f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [{'action': 'taking', 'product': 'gray-beer'},\n",
       "  {'action': 'taking', 'product': 'gray-beer'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'chips'}],\n",
       " '1': [{'action': 'taking', 'product': 'chips'},\n",
       "  {'action': 'taking', 'product': 'red-beer'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'chips'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'gray-beer'}],\n",
       " '2': [{'action': 'taking', 'product': 'red-beer'},\n",
       "  {'action': 'taking', 'product': 'red-beer'},\n",
       "  {'action': 'taking', 'product': 'yellow-beer'},\n",
       "  {'action': 'taking', 'product': 'chips'}]}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b0551-c6e8-49cb-96a8-ae6a6b72c591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8078e947-ff38-49fb-bed7-9b1f7789cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_camera = {\n",
    "    0: [],\n",
    "    1: [],\n",
    "    2: []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b86f3d59-f4b7-465a-9623-e25ca26ec738",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_range = (20, 60) # 62-100\n",
    "for image_dir in images_dir.iterdir():\n",
    "    name = image_dir.stem\n",
    "    camera = int(name.split(\"_\")[0])\n",
    "    n_frame = int(name.split(\"_\")[1])\n",
    "    if n_frame > filter_range[0] and n_frame < filter_range[1]:\n",
    "        images_camera[camera].append(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "dc8735c4-9b77-451c-ae4a-820b0330907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    images_camera[i] = sorted(images_camera[i], key=sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a6d038b3-7964-4d23-8dcc-2091e90bd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [path for i, path in enumerate(images_camera[0]) if i % 3 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bdca175a-2c61-4e32-88cf-713d7ff9da4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4c77f50b-c370-4d6c-9773-240804062e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_images = []\n",
    "for image_file_dir in sample:\n",
    "    base64_images.append(encode_image(image_file_dir))\n",
    "\n",
    "imgs_content = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_bs64}\"}} for image_bs64 in base64_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6cf4241c-6cd7-4afb-980b-4fa732e543a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.238897562026978\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "first_description = llm.invoke(\n",
    "                    [\n",
    "                        HumanMessage(\n",
    "                        content=[\n",
    "                            {\"type\": \"text\", \"text\": reflection_system_prompt}] + imgs_content\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7e6f8b20-5143-4da7-8a86-623bbe72fe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the provided image sequence, the customer performs the following actions:\\n\\n1. **Take Actions:**\\n   - The customer reaches for a product from the shelf containing canned beverages.\\n   - The customer picks up a can from the shelf and places it into a bag.\\n\\n2. **Return Actions:**\\n   - The customer does not appear to return any products to the shelf in the provided sequence.\\n\\n**Analysis:**\\n- The customer performs a total of 1 take action.\\n- There are no product return actions observed in the sequence.'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_description.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "cc3b4804-6799-470a-b8ef-b55a5570c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_analysis = [{\"type\": \"text\", \"text\": f\"## scene analysis:\\n {first_description.content}\\n Start to generate the structured output\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c42b6ca8-02b0-43e4-95dc-503634cd3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_structured =  llm.with_structured_output(schema=Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a262e9b1-f139-4c91-9744-342c6d058ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.975375652313232\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "structured_output = llm_structured.invoke(\n",
    "                    [\n",
    "                        HumanMessage(\n",
    "                        content=[\n",
    "                            {\"type\": \"text\", \"text\": sys_prompt}] + imgs_content + prev_analysis\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8c771ec0-69b3-4901-90a3-00cc93bdf28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Action(action='taking', product='yellow-beer')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_output.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7a20b-676b-460d-9d65-bb9ca9a75244",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee3bb8-1b4c-4756-9d1c-efa30e6738ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
